## EXP-2 – PROMPT ENGINEERING
## Aim

To perform a comparative analysis of different prompting patterns (broad/unstructured prompts vs. refined/structured prompts) and evaluate how responses vary in quality, accuracy, and depth across multiple test scenarios.

## Experiment

Prepare a set of scenarios/questions.

Frame prompts in two styles:

Broad / Unstructured Prompt – vague, general request.

Refined / Structured Prompt – specific, guided, and detailed.

Record the responses generated.

Compare based on clarity, accuracy, completeness, and relevance.

## Algorithm

Select test scenarios from different domains (e.g., education, coding, summarization, reasoning).

Frame two prompts for each: one broad and one refined.

Input prompts into the model and collect outputs.

Analyze responses on:

Relevance (matches the question intent).

Accuracy (factually correct).

Depth (detailed vs. superficial).

Clarity (well-structured vs. vague).

Compare broad vs. refined prompts for each scenario.

Summarize findings.

## Output
Scenario 1: Education – Explain Photosynthesis

Broad Prompt: “Tell me about photosynthesis.”

Response: General explanation, may miss chemical equation or details.

Refined Prompt: “Explain photosynthesis step by step, including the light-dependent and light-independent reactions with the chemical equation.”

Response: Detailed process with light reaction, Calvin cycle, balanced equation.

Comparison: Refined prompt gives structured, accurate, and complete answer.

Scenario 2: Coding – Sorting in C#

Broad Prompt: “Write sorting code.”

Response: May generate any sorting code (Python, Java, or C#), sometimes incomplete.

Refined Prompt: “Write a C# program to implement bubble sort on an integer array and explain each step.”

Response: Complete C# program with explanation of loops and comparisons.

Comparison: Refined prompt produces precise code in correct language with explanations.

Scenario 3: Summarization – News Article

Broad Prompt: “Summarize this news.”

Response: Brief, sometimes missing important context.

Refined Prompt: “Summarize this news article in 3 bullet points, highlighting the main issue, cause, and effect.”

Response: Clear, structured summary in bullet format with main highlights.

Comparison: Refined prompt improves clarity and focus.

Scenario 4: Reasoning – Math Problem

Broad Prompt: “Solve 2x + 5 = 15.”

Response: Provides answer but may skip steps.

Refined Prompt: “Solve the equation 2x + 5 = 15 step by step, showing how to isolate x.”

Response: Explains subtracting 5, dividing by 2, and final answer.

Comparison: Refined prompt improves reasoning clarity and learning value.

## Result

Broad/unstructured prompts → generate responses that are general, sometimes incomplete, and lacking depth.

Refined/structured prompts → produce more accurate, detailed, and relevant responses.

Comparative analysis shows that prompt specificity directly improves output quality in terms of clarity, accuracy, and depth.

Conclusion: Effective prompt engineering is crucial to fully utilize LLMs. Clear and structured prompts yield superior results compared to vague prompts.
